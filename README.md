# Project: Data Warehouse
## Introduction

A music streaming startup, Sparkify, has grown their user base and song database and wants to move their processes and data onto the cloud. Their data resides in S3, in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

As their data engineer, we are tasked with building an ETL pipeline that extracts their data from S3, stages them in Redshift, and transforms data into a set of dimensional tables for their analytics team to continue finding insights in what songs their users are listening to. 

## Datasets
Sparkify provided two datasets that reside in S3. Here are the S3 links for each:

    Song data: s3://udacity-dend/song_data
    Log data: s3://udacity-dend/log_data

### song Dataset

The first dataset is a subset of real data from the [Million Song Dataset](https://labrosa.ee.columbia.edu/millionsong/). Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are filepaths to two files in this dataset.

    song_data/A/B/C/TRABCEI128F424C983.json
    song_data/A/A/B/TRAABJL12903CDCF1A.json

And below is an example of what a single song file, TRAABJL12903CDCF1A.json, looks like.




`{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}`

### Log Dataset

The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate app activity logs from an imaginary music streaming app based on configuration settings.

The log files in the dataset you'll be working with are partitioned by year and month. For example, here are filepaths to two files in this dataset.

log_data/2018/11/2018-11-12-events.json

log_data/2018/11/2018-11-13-events.json

And below is an example of what the data in a log file, 2018-11-12-events.json, looks like.
![log data](/log-data.png)

## Schema for Song Play Analysis

A star schema is used for queries on song play analysis. It includes the following tables.
![sparkify star schema](/dend-project3-sparkify.PNG)


# Get STARTED
The project consists of the followling files:

**Redshift-cluster.ipynb**  ... The jupyter note is the scafolding that describes the build of a DWH cluster and is suplemented with some queries to test the database.

**dwh.cfg**  ... This is the configuratiuon file that contains all parameters required to build and run the cluster. The file is mainly empty and can be substituted with your own values.

**sql_queries.py**  ... is a python file that holds all DDL aand DML Statements to create and load the data.

**create_tables.py**  ... is a python file that calls all required functions to create the tables as described in the sql_queries.py 

**etl.py**  ... this is the python file that actually performs the ETL and therefore loads the staging data and further loads the actual tables used the perform queries on. 

## Create a AWS Cluster

open terminal and execute `jupyter notebook`

--> Open the file `Redshift-Cluster`, and execute each block by pressing `ctrl + Enter`

--> make sure you run the blocks in sequence
Block 1. Import the nessary libraries:
    pandas, dataframes

    json, json format
    
    boto3, aws cluster etc

Block 2. Load all Data ware house parameters which are stored in a configuration (param) file. In order to use the parameter file use your own AWS values

Block 3. create the clients or definitions from IAM, EC2, S3 and Redshift

Block 4. Now that the S3 bucket is defined, check the data (resources) that will be used to load our tables

Block 5. create the IAM role that provided the required access to the S3 bucket

Block 6. Finally the create the actual Redshift Cluster

Block 7. Check the availability of the cluster.

Block 8. **Once the cluster becomes available**, get the cluster endpoint and role ARN. 
Enter the endpoint as the host in the param file
enter the correct role (ARN) in the param file

Block 9. Open the incoming TCP port to access the cluster endpoint

Block 10. Load the SQL and connect to the database

Block 11. Once the sql is loaded and you are able to connect to the database, the run two Python scripts from a normal terminal window

`python create_table.py` creates all necessary table to load the data in

`python etl.py` opens first the S3 bucket. The command COPY is used to read all JSON files in the bucket and to load the staging tables. After the staging tables are loaded, the ETL is filling the necessary tables that can be used to run queries against. To test and finish the project in time, I decided to filter the songs directory to only get data from 's3://udacity-dend/song-data/A'. Until now I wasn't able to load the entire dataset!

You also find some queries to test the data loaded in the star diagram table 

* The first queries are row counts of each table
* How many Paid vs free users are there
* List all artist based on popularity
* List all songs based on popularity
* What is the number of male and female users
* Most popular weekday for our sparkify

